dataset_path: "../../../../../../srv/scratch/bic/peter/Report"
histocartography_path: "../histocartography/histocartography"
graph_path: "../../../../../../srv/scratch/bic/peter/full-graph-raw"
encoder_path: "../../../../../../srv/scratch/bic/peter/encoder"
decoder_path: "../../../../../../srv/scratch/bic/peter/decoder"
output_path: "output"
vocab_path: "new_vocab_bladderreport.pkl" # process_vocab_bladderreport.pkl
learning_rate: 0.0015
weight_decay: 0.001
phase: train # train, test, eval
epochs: 200
decoder_type: "LSTM" # Transformer, LSTM
train_mode: "train_mode"
encoder: "GCN"
graph_model_type: "Hierarchical" #  Hierarchical, Cell, Tissue
optimizer_type: "Adam" # Adam, SGD
batch_size: 32
eval_batch_size: "full"
model_save_base_path: "../../../../../../srv/scratch/bic/peter/model_save"
save_model: True
save_encoder: "Encoder-54" # put encoder name
save_decoder: "Decoder-54" # put decoder name 
save_global_feature_extractor: "GlobalExtractor-54" # put feature extractor name
save_classifier: "Classifier1" # 
save_every: 2
loss: label # Both, caption, label 
wandb_name: "54 lr 0.001 LSTM 1 layer 256 hid scheduler"
load_encoder_name : "Encoder-52-50-2.94.pt"
load_global_extractor_name: "GlobalExtractor-52-50-2.94.pt"
load_decoder: "Decoder-52-50-2.94.pt"
load_train: False
classifier_name: "Normal Way to do it 8"
load_embedding: False
embed_model_path: "../../../../../../srv/scratch/bic/peter/CLIP_save/best_256.pt"
trainable_embedding: False
label_loss_weight: 25

gnn_param:
  cell_layers: 3
  tissue_layers: 2
  cell_conv_method: "GIN" # GCN, GAT, GraphSage, GIN, PNA
  tissue_conv_method: "GIN" # GCN, GAT, GraphSage, GIN, PNA
  pool_method: None # None, Diff, MinCut
  aggregate_method: "mean" # sum, mean, max
  hidden_size: 256
  output_size: 256
  GAT: 
    num_heads: 2
  GraphSage:
    aggregator_type: "gcn" # mean, gcn, pool, lstm
  GIN:

global_class_param:
  hidden_size: 256
  output_size: 256
  dropout_rate: 0.5

classifier_param:
  num_class: 3
  hidden_size: 128
  dropout_rate: 0.5

transformer_param:
  n_head : 4
  num_layers: 1
  dim_feedforward: 2048
  dropout: 0.4


lstm_param:
  dropout: 0.5 # too much dropout in lstm breaks it
  num_layers: 1
  size: 256
  bi_direction: False

