{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "243b5e9a-d6d1-4e19-9c43-c9d0fb2598fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/pbs.5152609.kman.restech.unsw.edu.au/ipykernel_270318/1573852760.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[nltk_data] Downloading package wordnet to /home/z5313504/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from IPython import display as ipythondisplay\n",
    "from torch import nn\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import cv2\n",
    "import gc\n",
    "import math\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from dataloader import make_dataloader,dataset_to_loader\n",
    "from models.CLIP import CLIPModel, AvgMeter\n",
    "from models.Graph_Model import GNNEncoder\n",
    "from models.LSTM2 import LSTMDecoder\n",
    "from models.GlobalFeatureExtractor import GlobalFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e8c89e-9dc4-4080-8cce-36f269136eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Code\n",
    "def get_image_embeddings(loader, model_path,device):\n",
    "    # tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(CFG.clinical_encoder_model)\n",
    "    config_file_path =  \"config/config.yaml\"\n",
    "    with open(config_file_path, \"r\") as conf_file:\n",
    "        args = yaml.full_load(conf_file)\n",
    "    \n",
    "    # model = CLIPModel(args,device).to(device)\n",
    "    model = torch.load(model_path, map_location=device)\n",
    "    model.eval()\n",
    "    \n",
    "    valid_image_embeddings = []\n",
    "    \n",
    "    total_samples = len(loader.dataset)\n",
    "\n",
    "    total_step = math.ceil(total_samples / loader.batch_size) \n",
    "    with torch.no_grad():\n",
    "        for _ in range(total_step):\n",
    "            cg, tg, assign_mat, caption_tokens, labels, caption, images,_ = next(iter(loader))\n",
    "            cg = cg.to(device)\n",
    "            tg = tg.to(device)\n",
    "            images = images.to(device)\n",
    "            caption_tokens = caption_tokens.to(device) \n",
    "            graph_out = model.graph_encoder(cg,tg,assign_mat,images)\n",
    "            global_feat = model.feature_extractor(images)\n",
    "            merged_feat = torch.cat((graph_out, global_feat), dim=1)\n",
    "            image_embeddings = model.image_projection(merged_feat)\n",
    "            valid_image_embeddings.append(image_embeddings)\n",
    "    return model, torch.cat(valid_image_embeddings)\n",
    "\n",
    "def get_text_embeddings(loader,model_path,device):\n",
    "    config_file_path =  \"config/config.yaml\"\n",
    "    with open(config_file_path, \"r\") as conf_file:\n",
    "        args = yaml.full_load(conf_file)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\")\n",
    "    \n",
    "    model = CLIPModel(args,device).to(device)\n",
    "    model = torch.load(model_path, map_location=device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_samples = len(loader.dataset)\n",
    "\n",
    "    total_step = math.ceil(total_samples / loader.batch_size) \n",
    "    valid_text_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(total_step):\n",
    "            cg, tg, assign_mat, caption_tokens, labels, caption, images,attention_masks = next(iter(loader))\n",
    "            # cg = cg.to(device)\n",
    "            # tg = tg.to(device)\n",
    "            # images = images.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            caption_tokens = caption_tokens.to(device) \n",
    "            text_features = model.text_encoder(\n",
    "                caption_tokens,attention_masks\n",
    "            )\n",
    "            text_embeddings = model.text_projection(text_features)\n",
    "            valid_text_embeddings.append(text_embeddings)\n",
    "    return model, torch.cat(valid_text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0138ba-a859-474a-8dab-d88233a7b492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../../../../../srv/scratch/bic/peter/full-graph/cell_graphs/train/115833_000.bin', '../../../../../../srv/scratch/bic/peter/full-graph/cell_graphs/train/115833_001.bin']\n",
      "length report 2336\n",
      "['../../../../../../srv/scratch/bic/peter/full-graph/cell_graphs/test/115831_000.bin', '../../../../../../srv/scratch/bic/peter/full-graph/cell_graphs/test/115831_001.bin']\n",
      "length report 998\n",
      "['../../../../../../srv/scratch/bic/peter/full-graph/cell_graphs/eval/115831_002.bin', '../../../../../../srv/scratch/bic/peter/full-graph/cell_graphs/eval/115831_003.bin']\n",
      "length report 889\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dl,train_dataset = make_dataloader(\n",
    "        batch_size = 16,\n",
    "        split = \"train\",\n",
    "        base_data_path = \"../../../../../../srv/scratch/bic/peter/Report\",\n",
    "        graph_path = \"../../../../../../srv/scratch/bic/peter/full-graph\",\n",
    "        vocab_path = \"new_vocab_bladderreport.pkl\",\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        load_in_ram = True\n",
    "    )\n",
    "    # #train_dl = get_sample_samplier(train_dataset,args[\"batch_size\"])\n",
    "    # print(f\"train loader size {len(train_dl)}\")\n",
    "\n",
    "test_dl,_ = make_dataloader(\n",
    "        batch_size =16, # there are 1000 set 1 because we will calculate pair by pair\n",
    "        split = \"test\",\n",
    "        base_data_path = \"../../../../../../srv/scratch/bic/peter/Report\",\n",
    "        graph_path = \"../../../../../../srv/scratch/bic/peter/full-graph\",\n",
    "        vocab_path = \"new_vocab_bladderreport.pkl\",\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        load_in_ram = True\n",
    "    )\n",
    "\n",
    "eval_dl,_ = make_dataloader(\n",
    "        batch_size = 16, # there are 889 in eval set\n",
    "        split = \"eval\",\n",
    "        base_data_path = \"../../../../../../srv/scratch/bic/peter/Report\",\n",
    "        graph_path = \"../../../../../../srv/scratch/bic/peter/full-graph\",\n",
    "        vocab_path = \"new_vocab_bladderreport.pkl\",\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        load_in_ram = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6242ccd-ce51-47b3-90de-8f48eaeea1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_128.pt  best_256.pt  best_64.pt  best.pt\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../../../../srv/scratch/bic/peter/CLIP_save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f28b95c-a891-4dbc-b9ae-0df2afcda356",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path =  \"config/config.yaml\"\n",
    "with open(config_file_path, \"r\") as conf_file:\n",
    "    args = yaml.full_load(conf_file)\n",
    "\n",
    "# model = CLIPModel(args,device).to(device)\n",
    "model = torch.load(\"../../../../../../srv/scratch/bic/peter/CLIP_save/best_128.pt\", map_location=device)\n",
    "for name, param in model.named_parameters():\n",
    "    #if 'fc' not in name:  # or any other condition based on layer names\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc92b1c9-623e-4733-8587-0fe7e9373c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([896, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, image_embeddings = get_image_embeddings(eval_dl, \"../../../../../../srv/scratch/bic/peter/CLIP_save/best_64.pt\",device)\n",
    "print(image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6083990-8b08-4668-81c9-656656d55ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([896, 512])\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model, text_embeddings = get_text_embeddings(eval_dl,\"../../../../../../srv/scratch/bic/peter/CLIP_save/best_64.pt\",device)\n",
    "print(text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a69a0-06cb-4f89-85a3-febb2e9ff0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b966c6cc-8a11-4b63-94b7-4721b14c6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import os\n",
    "    print(\"DATALOADER\")\n",
    "    split = \"eval\"\n",
    "    splits = [\"train\",\"test\",\"eval\"]\n",
    "    loader,_ = make_dataloader(\n",
    "        batch_size = 2,\n",
    "        split = split,\n",
    "        base_data_path = \"../../../../../../srv/scratch/bic/peter/Report\",\n",
    "        graph_path = \"../../../../../../srv/scratch/bic/peter/full-graph\",\n",
    "        vocab_path = \"new_vocab_bladderreport.pkl\",\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        load_in_ram = True\n",
    "    )\n",
    "    # for idx,output in enumerate(dataset):\n",
    "    #     _, _, _, _, labels, _, _= output\n",
    "    #     print(labels)\n",
    "    #     class_count[str(labels)] += 1\n",
    "    #     count += 1\n",
    "\n",
    "    print(f\"length data loader for {split} is {len(loader)}\")\n",
    "    for batch_idx, batch_data in enumerate(loader):\n",
    "    # Your batch processing code here\n",
    "        cg, tg, assign_mat, caption_tokens, labels, caption, images, attention_masks= batch_data\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\")\n",
    "        enc_cap = tokenizer(\n",
    "            caption, padding=True, truncation=True, max_length=90\n",
    "        )\n",
    "        print(caption)\n",
    "        # print(enc_cap[\"pad_token_id\"])\n",
    "        print(enc_cap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 (ipykernel)",
   "language": "python",
   "name": "python-3.10.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
