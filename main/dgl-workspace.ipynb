{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5622d9cc-554c-43e6-8aef-7c8c2b8a2f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterhuang/Library/Python/3.8/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "g = dgl.graph(([0, 0, 0, 0, 0], [1, 2, 3, 4, 5]), num_nodes=6)\n",
    "# Equivalently, PyTorch LongTensors also work.\n",
    "g = dgl.graph(\n",
    "    (torch.LongTensor([0, 0, 0, 0, 0]), torch.LongTensor([1, 2, 3, 4, 5])),\n",
    "    num_nodes=6,\n",
    ")\n",
    "\n",
    "# You can omit the number of nodes argument if you can tell the number of nodes from the edge list alone.\n",
    "g = dgl.graph(([0, 0, 0, 0, 0], [1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36389ce-ede1-49fb-b304-62e05e27ee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0610, -2.6291, -1.5116,  1.2287],\n",
      "        [ 0.4833, -1.6545, -1.1173,  0.0300],\n",
      "        [ 0.0089,  0.4669,  0.4350,  1.2601],\n",
      "        [ 0.7695,  2.0895, -1.7074, -0.5123],\n",
      "        [ 0.0142, -0.0962, -1.2562, -0.8136]])\n"
     ]
    }
   ],
   "source": [
    "# Assign a 3-dimensional node feature vector for each node.\n",
    "g.ndata[\"x\"] = torch.randn(6, 3)\n",
    "# Assign a 4-dimensional edge feature vector for each edge.\n",
    "g.edata[\"a\"] = torch.randn(5, 4)\n",
    "# Assign a 5x4 node feature matrix for each node.  Node and edge features in DGL can be multi-dimensional.\n",
    "g.ndata[\"y\"] = torch.randn(6, 5, 4)\n",
    "\n",
    "print(g.edata[\"a\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39445269-ba9d-4047-a40f-d9918315a31f",
   "metadata": {},
   "source": [
    "## Quantifying Graph Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e98920-a056-44af-8e83-2a8a2a48861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "5\n",
      "5\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(g.num_nodes())\n",
    "print(g.num_edges())\n",
    "# Out degrees of the center node\n",
    "print(g.out_degrees(0))\n",
    "# In degrees of the center node - note that the graph is directed so the in degree should be 0.\n",
    "print(g.in_degrees(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce12075-939f-4540-b676-efeb4a7c9182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Induce a subgraph from node 0, node 1 and node 3 from the original graph.\n",
    "sg1 = g.subgraph([0, 1, 3])\n",
    "# Induce a subgraph from edge 0, edge 1 and edge 3 from the original graph.\n",
    "sg2 = g.edge_subgraph([0, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23feaf4c-c165-4e6f-829d-09d68424972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "torch.Size([2, 4])\n",
      "tensor([[-0.3360, -0.9976, -2.4161],\n",
      "        [ 2.4269, -0.9421,  0.6361],\n",
      "        [ 1.2461,  0.8691,  1.0383],\n",
      "        [ 0.8714, -0.4506, -0.8767]])\n",
      "tensor([[-1.0610, -2.6291, -1.5116,  1.2287],\n",
      "        [ 0.4833, -1.6545, -1.1173,  0.0300],\n",
      "        [ 0.7695,  2.0895, -1.7074, -0.5123]])\n"
     ]
    }
   ],
   "source": [
    "# The original node feature of each node in sg1\n",
    "print(sg1.ndata[\"x\"].shape)\n",
    "# The original edge feature of each node in sg1\n",
    "print(sg1.edata[\"a\"].shape)\n",
    "# The original node feature of each node in sg2\n",
    "print(sg2.ndata[\"x\"])\n",
    "# The original edge feature of each node in sg2\n",
    "print(sg2.edata[\"a\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ccaefb-0a54-4bd5-8bc3-25843a944a33",
   "metadata": {},
   "source": [
    "## Save and Load Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d5df9-bfa1-4f5d-aeed-615b0f49f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl.save_graphs(\"graph.dgl\", g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb496f-89a4-4c48-8672-8bafb540d5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Graph(num_nodes=6, num_edges=5,\n",
      "      ndata_schemes={'y': Scheme(shape=(5, 4), dtype=torch.float32), 'x': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'a': Scheme(shape=(4,), dtype=torch.float32)})]\n",
      "-----\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "a,b = dgl.load_graphs(\"graph.dgl\")\n",
    "print(a)\n",
    "print(\"-----\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d23b651-bd29-4013-9dd4-a189982c1f6b",
   "metadata": {},
   "source": [
    "## Custom GNN for Graph Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8f4e3c-9f31-45ca-b516-054509f4efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import dgl\n",
    "import dgl.data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9250874-c28b-441f-836e-f958c1ecbd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/featurize/.dgl/GINDataset.zip from https://raw.githubusercontent.com/weihua916/powerful-gnns/master/dataset.zip...\n",
      "Extracting file to /home/featurize/.dgl/GINDataset\n"
     ]
    }
   ],
   "source": [
    "# Generate a synthetic dataset with 10000 graphs, ranging from 10 to 500 nodes.\n",
    "dataset = dgl.data.GINDataset(\"PROTEINS\", self_loop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec1e70-3db5-4a0c-8893-ae44b42534b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feature dimensionality: 3\n",
      "Number of graph categories: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Node feature dimensionality:\", dataset.dim_nfeats)\n",
    "print(\"Number of graph categories:\", dataset.gclasses)\n",
    "\n",
    "\n",
    "from dgl.dataloading import GraphDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff3305-2c75-41ed-b32a-cd51f120da20",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GINDataset' object has no attribute 'graph_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_label\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GINDataset' object has no attribute 'graph_label'"
     ]
    }
   ],
   "source": [
    "print(dataset.graph_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564bdbe-60ac-4702-81af-f3214fd93447",
   "metadata": {},
   "source": [
    "### Define a Graph Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33111905-1b07-4e83-b062-796e5b1f2cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dbcde5-3035-4fe9-9bb7-cbbdebfbe5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_examples = len(dataset)\n",
    "num_train = int(num_examples * 0.8)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(torch.arange(num_train))\n",
    "test_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))\n",
    "\n",
    "train_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=train_sampler, batch_size=5, drop_last=False\n",
    ")\n",
    "test_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=test_sampler, batch_size=5, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95342d6-bd85-45f6-ab93-f830ae741e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Graph(num_nodes=202, num_edges=892,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), tensor([0, 0, 1, 0, 1])]\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_dataloader)\n",
    "batch = next(it)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8f93b-ca39-4c51-b173-8bb02ecb5c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes for each graph element in the batch: tensor([30, 52, 13, 84, 23])\n",
      "Number of edges for each graph element in the batch: tensor([142, 250,  65, 320, 115])\n",
      "The original graphs in the minibatch:\n",
      "[Graph(num_nodes=30, num_edges=142,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=52, num_edges=250,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=13, num_edges=65,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=84, num_edges=320,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=23, num_edges=115,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={})]\n"
     ]
    }
   ],
   "source": [
    "batched_graph, labels = batch\n",
    "print(\n",
    "    \"Number of nodes for each graph element in the batch:\",\n",
    "    batched_graph.batch_num_nodes(),\n",
    ")\n",
    "print(\n",
    "    \"Number of edges for each graph element in the batch:\",\n",
    "    batched_graph.batch_num_edges(),\n",
    ")\n",
    "\n",
    "# Recover the original graph elements from the minibatch\n",
    "graphs = dgl.unbatch(batched_graph)\n",
    "print(\"The original graphs in the minibatch:\")\n",
    "print(graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c91086-6f49-499b-a8fa-ee578a54b68e",
   "metadata": {},
   "source": [
    "### Define Simple GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75a36b-81f4-4cb4-8e5f-f0412c89cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        g.ndata[\"h\"] = h\n",
    "        return dgl.mean_nodes(g, \"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f14c5-8a19-4872-ae49-4633b4dc5859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  hact-net\twork\n"
     ]
    }
   ],
   "source": [
    "!ls ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ccdbe-75f0-41f1-952a-73da3cca690e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in the directory: 4253\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = \"../../data/Images\"  # Replace with the actual directory path\n",
    "\n",
    "# List all items (files and subdirectories) in the directory\n",
    "items = os.listdir(directory)\n",
    "\n",
    "# Filter out only the files from the list of items\n",
    "files = [item for item in items if os.path.isfile(os.path.join(directory, item))]\n",
    "\n",
    "num_files = len(files)\n",
    "print(\"Number of files in the directory:\", num_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34436967-8201-457f-9be3-d226b8637d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Inference.ipynb\t    main\t\t\t    real-time.ipynb\n",
      " augment.ipynb\t\t    main.py\t\t\t   'resnet (1).py'\n",
      " dev_18.90_PHOENIX14-T.pt   main_copy.py\t\t    resnet.py\n",
      " hands-on-session-2\t    out.output-hypothesis-dev.ctm   session.zip\n",
      " histocartography\t    output_video-1.mp4\n",
      " histocartography.zip\t    output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "!ls ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21b343-51c4-4266-9ff8-a271e9adde93",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# import sys\n",
    "# sys.path.append('../histocartography/histocartography')  # Add the parent_folder to the Python path\n",
    "# from preprocessing import (\n",
    "#     VahadaneStainNormalizer,         # stain normalizer\n",
    "#     NucleiExtractor,                 # nuclei detector \n",
    "#     DeepFeatureExtractor,            # feature extractor \n",
    "#     KNNGraphBuilder,                 # kNN graph builder\n",
    "#     ColorMergedSuperpixelExtractor,  # tissue detector\n",
    "#     DeepFeatureExtractor,            # feature extractor\n",
    "#     RAGGraphBuilder,                 # build graph\n",
    "#     AssignmnentMatrixBuilder         # assignment matrix \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e03454-428a-4957-bb5f-a6758ad9f233",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "store_path = \"graph\"\n",
    "split = \"test\"\n",
    "image_name = \"1_00061_sub0_007\"\n",
    "cg_out = os.path.join(store_path, 'cell_graphs', split, image_name.replace('.png', '.bin'))\n",
    "tg_out = os.path.join(store_path, 'tissue_graphs', split, image_name.replace('.png', '.bin'))\n",
    "assign_out = os.path.join(store_path, 'assignment_matrices', split, image_name.replace('.png', '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6570f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cg_ou)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef141498",
   "metadata": {},
   "source": [
    "## Read an exmaple graph to have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a2df83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_00061_sub0_002.h5 1_00061_sub0_004.h5\n"
     ]
    }
   ],
   "source": [
    "!ls graph/assignment_mat/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0b5b3ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['feat', 'centroid'])\n"
     ]
    }
   ],
   "source": [
    "cell_path = \"graph/cell_graphs/test/1_00061_sub0_002.bin\"\n",
    "tissue_path = \"graph/tissue_graphs/test/1_00061_sub0_002.bin\"\n",
    "assignment_path = \"graph/assignment_mat/test/1_00061_sub0_002.h5\"\n",
    "ass_two = \"graph/assignment_mat/test/1_00061_sub0_004.h5\"\n",
    "assignment_paths  = [\"graph/assignment_mat/test/1_00061_sub0_002.h5\",\"graph/assignment_mat/test/1_00061_sub0_004.h5\"]\n",
    "loaded_graphs = dgl.load_graphs(cell_path)\n",
    "print(loaded_graphs[0][0].ndata.keys())\n",
    "cg_feat = loaded_graphs[0][0].ndata['feat']\n",
    "cg_centroid = loaded_graphs[0][0].ndata['centroid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "11a32716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['feat', 'centroid'])\n"
     ]
    }
   ],
   "source": [
    "loaded_graphs = dgl.load_graphs(tissue_path)\n",
    "print(loaded_graphs[0][0].ndata.keys())\n",
    "tg_feat = loaded_graphs[0][0].ndata['feat']\n",
    "tg_centroid = loaded_graphs[0][0].ndata['centroid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "87a8165c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['assignment_matrix']>\n",
      "(387, 9)\n",
      "<HDF5 dataset \"assignment_matrix\": shape (387, 9), type \"<f8\">\n",
      "(387,)\n",
      "Value 0: Count 1\n",
      "Value 1: Count 60\n",
      "Value 2: Count 32\n",
      "Value 3: Count 1\n",
      "Value 4: Count 43\n",
      "Value 5: Count 53\n",
      "Value 6: Count 189\n",
      "Value 7: Count 6\n",
      "Value 8: Count 2\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "    # Assuming the assignment matrix is stored in a dataset named \"assignment\"\n",
    "with h5py.File(assignment_path, \"r\") as f:\n",
    "    print(f.keys())\n",
    "    print(f[\"assignment_matrix\"].shape)\n",
    "    ass_mat = f[\"assignment_matrix\"]\n",
    "    print(ass_mat)\n",
    "\n",
    "    # Find the column number with value 1 in each row\n",
    "    column_numbers = np.argmax(ass_mat, axis=1)\n",
    "\n",
    "    # Print the column numbers\n",
    "    # print(column_numbers)\n",
    "    print(column_numbers.shape)\n",
    "\n",
    "    value_counts = np.bincount(column_numbers, minlength=5)\n",
    "\n",
    "    # Print the count for each value\n",
    "    for value, count in enumerate(value_counts):\n",
    "        print(f\"Value {value}: Count {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fc5cbfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "graph/assignment_mat/test/1_00061_sub0_002.h5\n",
      "FDSFS\n",
      "Individual shape (387, 9)\n",
      "graph/assignment_mat/test/1_00061_sub0_004.h5\n",
      "FDSFS\n",
      "Individual shape (290, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIndividual shape \u001b[39m\u001b[39m{\u001b[39;00massignment_matrix\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     assignment_matrices\u001b[39m.\u001b[39mappend(assignment_matrix)\n\u001b[0;32m---> 19\u001b[0m assignment_tensor \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mstack(assignment_matrices, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(assignment_tensor\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     21\u001b[0m \u001b[39m# Find the column number with value 2 in each row\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/numpy/core/shape_base.py:464\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    462\u001b[0m shapes \u001b[39m=\u001b[39m {arr\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays}\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shapes) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mall input arrays must have the same shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    466\u001b[0m result_ndim \u001b[39m=\u001b[39m arrays[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mndim \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    467\u001b[0m axis \u001b[39m=\u001b[39m normalize_axis_index(axis, result_ndim)\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "# List of HDF5 file paths\n",
    "# assignment_paths = [\n",
    "#     \"graph/assignment_mat/test/1_00061_sub0_004.h5\",\n",
    "#     \"path_to_another_file.h5\",  # Add more file paths as needed\n",
    "# ]\n",
    "assignment_paths  = [assignment_path,\"graph/assignment_mat/test/1_00061_sub0_004.h5\"]\n",
    "print(len(assignment_paths))\n",
    "assignment_matrices = []\n",
    "for assignment_path in assignment_paths:\n",
    "    # Open the HDF5 file for reading\n",
    "    print(assignment_path)\n",
    "    with h5py.File(assignment_path, \"r\") as file:\n",
    "        print(\"FDSFS\")\n",
    "        # Assuming the assignment matrix is stored in a dataset named \"assignment_matrix\"\n",
    "        assignment_matrix = file[\"assignment_matrix\"][:]\n",
    "        \n",
    "    print(f\"Individual shape {assignment_matrix.shape}\")\n",
    "    assignment_matrices.append(assignment_matrix)\n",
    "assignment_tensor = np.stack(assignment_matrices, axis=0)\n",
    "print(assignment_tensor.shape)\n",
    "# Find the column number with value 2 in each row\n",
    "column_numbers = np.argmax(assignment_tensor, axis=2)\n",
    "\n",
    "# Print the column_numbers shape\n",
    "print(f\"Shape of column_numbers {column_numbers.shape}\")\n",
    "# for i in range(column_numbers.shape[0]):\n",
    "#     print(i.shape)\n",
    "# print(column_numbers[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a399fafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Cell graph have feature size torch.Size([387, 514]) and centroid size torch.Size([387, 2])\n",
      "Tissue graph have feature size torch.Size([9, 514]) and centroid size torch.Size([9, 2])\n",
      "Assignment matrix size (387, 9)\n",
      "Column vector shape (387,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary:\")\n",
    "print(f\"Cell graph have feature size {cg_feat.size()} and centroid size {cg_centroid.size()}\")\n",
    "print(f\"Tissue graph have feature size {tg_feat.size()} and centroid size {tg_centroid.size()}\")\n",
    "print(f\"Assignment matrix size {ass_mat.shape}\")\n",
    "print(f\"Column vector shape { column_numbers.shape}\")\n",
    "print(type(column_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7af8bf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([387])\n"
     ]
    }
   ],
   "source": [
    "values = torch.from_numpy(column_numbers)\n",
    "print(values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "75917359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 514])\n"
     ]
    }
   ],
   "source": [
    "summed_features = torch.zeros(tg_feat.shape)  # Assuming there are 10 classes\n",
    "print(summed_features.shape)\n",
    "# Use torch.scatter_add to accumulate features based on class labels\n",
    "summed_features = torch.scatter_add(summed_features, dim=0, index=torch.from_numpy(column_numbers).unsqueeze(1), src=cg_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8c972ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d71a8f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cell_layers': 2, 'tissue_layers': 2, 'cell_conv_method': 'GIN'}\n"
     ]
    }
   ],
   "source": [
    "path = \"config/config.yaml\"\n",
    "import yaml\n",
    "with open(path, 'r') as file:\n",
    "    configs = yaml.safe_load(file)\n",
    "print(configs[\"gnn_param\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
