{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d24b56-02a1-44ec-ba0a-896238ea93bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit shape torch.Size([16, 4])\n",
      "logit shape torch.Size([16])\n",
      "tensor(1.7956)\n",
      "loss shape torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "batch_size = 16\n",
    "# Your model's output logits, shape (batch_size, 4)\n",
    "logits = torch.randn(batch_size, 4)  # Replace with your actual logits\n",
    "print(f\"logit shape {logits.shape}\")\n",
    "# Ground truth labels as class indices, shape (batch_size,)\n",
    "labels = torch.randint(4, (batch_size,))  # Replace with your actual labels\n",
    "print(f\"logit shape {labels.shape}\")\n",
    "# Define the Cross-Entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = criterion(logits, labels)\n",
    "print(loss)\n",
    "print(f\"loss shape {loss.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e5e76b-ea32-4e87-8c72-9983fb84109a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 3, 3, 0, 1, 2, 0, 1, 1, 1, 3, 3, 3, 2, 0])\n",
      "tensor([1, 2, 0, 2, 2, 3, 1, 3, 1, 0, 0, 3, 2, 2, 0, 3])\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = torch.argmax(logits, dim=1)\n",
    "print(predicted_labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a31bbc-5d90-40e9-98c2-fe5f27a62edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3125\n",
      "0.3010989010989011\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "correct_predictions = (predicted_labels == labels).sum().item()\n",
    "accuracy = correct_predictions / batch_size\n",
    "f1 = f1_score(labels, predicted_labels, average='weighted') \n",
    "print(accuracy)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deb137b8-cdcc-4274-9a51-ae1fc93195a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the configuration from the YAML file\n",
    "with open('config/config.yaml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe43c85-bc43-4f8e-ba48-e56d29c01945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.LSTM import LSTMDecoder\n",
    "from models.Attention import EncoderLayer\n",
    "from simple_parsing import ArgumentParser\n",
    "import torch\n",
    "import torch.nn\n",
    "from Vocabulary import Vocabulary\n",
    "from models.Graph_Model import GNNEncoder\n",
    "args = config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = GNNEncoder(\n",
    "    args = args,\n",
    "    cg_layer = 3,\n",
    "    tg_layer = 3,\n",
    "    aggregate_method = \"mean\", \n",
    "    input_feat = 514,\n",
    "    hidden_size = 128,\n",
    "    output_size = 128,\n",
    ")\n",
    "decoder = LSTMDecoder(\n",
    "    vocab_size = 119, \n",
    "    embed_size = 512, \n",
    "    hidden_size = 512,  \n",
    "    batch_size= args[\"batch_size\"], \n",
    "    bi_direction = args[\"lstm_param\"][\"bi_direction\"],\n",
    "    device = device,\n",
    "    dropout = args[\"lstm_param\"][\"dropout\"],\n",
    "        num_layers = args[\"lstm_param\"][\"num_layers\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54cd5b8f-b200-4171-b312-ec0c54d7a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(encoder.softmax.parameters()))\n",
    "# print(encoder.parameters)\n",
    "\n",
    "encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "# Create a list of parameters, excluding the final softmax layer\n",
    "parameters_to_optimize = [p for p in encoder_parameters if p is not encoder.softmax.weight and p is not encoder.softmax.bias]\n",
    "# print(parameters_to_optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc752de-694f-4fa4-b5f7-e37137e971f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length data loader for train is 5840\n",
      "torch.Size([2, 3, 500, 500])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from Vocabulary import Vocabulary\n",
    "from dataloader import make_dataloader\n",
    "split = \"train\"\n",
    "loader,_ = make_dataloader(\n",
    "    batch_size = 2,\n",
    "    split = split,\n",
    "    base_data_path = \"../../../../../../srv/scratch/bic/peter/Report\",\n",
    "    graph_path = \"../../../../../../srv/scratch/bic/peter/full-graph\",\n",
    "    vocab_path = \"vocab_bladderreport.pkl\",\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    load_in_ram = True\n",
    ")\n",
    "print(f\"length data loader for {split} is {len(loader)}\")\n",
    "\n",
    "# for word, idx in loader.dataset.vocab.word2idx.items():\n",
    "#     print(f\"Word: {word}, Index: {idx}\")\n",
    "cg,tg,am,cap_to, label, cap, img = next(iter(loader))\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "202a67c9-110e-4df5-9d55-9294e8fd5959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z5313504/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/z5313504/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "resnet34 = torchvision.models.resnet34(pretrained=True)\n",
    "with torch.no_grad():\n",
    "    output = resnet34(img)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b087e-f42d-48d6-94dc-43a3d7ec3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../../../../../srv/scratch/bic/peter/Report/Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ed39a-32bf-4627-b220-034f3e6d2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob \n",
    "all_img = glob(os.path.join(\"../../../../../../srv/scratch/bic/peter/Report/Images/train\",\"*.png\"))\n",
    "print(len(all_img))\n",
    "print(all_img[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d47a8-096b-4f0f-be9e-221e29ae56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "# Load the image using OpenCV\n",
    "image = Image.open(all_img[0]).convert('RGB')\n",
    "print(type(image))\n",
    "print(image.size)\n",
    "\n",
    "plt.imshow(image)  # OpenCV uses BGR format, so we convert it to RGB for Matplotlib\n",
    "plt.title(\"Image\")\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc0ba2-47c6-4930-a113-f4f1f3f05b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "resnet34 = torchvision.models.resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a3c480-b98f-4c36-b692-2494a9044237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((500, 500)),  # Resize to the desired size\n",
    "    transforms.ToTensor(),  # Convert to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet statistics\n",
    "])\n",
    "\n",
    "# Apply the preprocessing steps to the image\n",
    "print(type(image))\n",
    "input_tensor = preprocess(image)\n",
    "print(input_tensor.shape)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "with torch.no_grad():\n",
    "    output = resnet34(input_batch)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd20205-a8a1-468c-8182-90feb35b334c",
   "metadata": {},
   "source": [
    "### Example for nn.TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5db0673-3fa7-4e16-bf20-baba63e71c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the parameters\n",
    "d_model = 512  # Model dimension\n",
    "nhead = 8     # Number of attention heads\n",
    "num_layers = 6  # Number of decoder layers\n",
    "dim_feedforward = 2048  # Feedforward dimension\n",
    "\n",
    "# Create the decoder\n",
    "dec = nn.TransformerDecoder(\n",
    "    nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward),\n",
    "    num_layers,\n",
    ")\n",
    "\n",
    "# Input and target sequences\n",
    "tgt = torch.rand(10, 32, d_model)  # (sequence_length, batch_size, d_model)\n",
    "memory = torch.rand(1, 32, d_model)  # (sequence_length, batch_size, d_model)\n",
    "\n",
    "# Output from the decoder\n",
    "output = dec(tgt, memory)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93af2b9c-62d7-4e11-95fd-21ccd2673644",
   "metadata": {},
   "source": [
    "## Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a6a1d0-6d74-4084-ab80-3b10222e5c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Graph_Model import GNNEncoder\n",
    "from models.LSTM import LSTMDecoder\n",
    "from models.GlobalFeatureExtractor import GlobalFeatureExtractor\n",
    "from models.Classifier import Classifier\n",
    "from models.Transformer import TransformerDecoder\n",
    "from models.Attention import EncoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762c346-8ff7-4446-8fe9-32dbaf761bfc",
   "metadata": {},
   "source": [
    "### Load a data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fdc6c73-0210-446a-829f-19251a1687bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length data loader for train is 1168\n",
      "caption token torch.Size([2, 90])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataloader import make_dataloader\n",
    "from Vocabulary import Vocabulary\n",
    "split = \"train\"\n",
    "loader,dataset = make_dataloader(\n",
    "    batch_size = 2,\n",
    "    split = split,\n",
    "    base_data_path = \"../../../../../../srv/scratch/bic/peter/Report\",\n",
    "    graph_path = \"../../../../../../srv/scratch/bic/peter/full-graph\",\n",
    "    vocab_path = \"vocab_bladderreport.pkl\",\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    load_in_ram = True\n",
    ")\n",
    "print(f\"length data loader for {split} is {len(loader)}\")\n",
    "cg, tg, assign_mat, caption_tokens, labels, caption, images = next(iter(loader))\n",
    "print(f\"caption token {caption_tokens.shape}\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4024b1b0-3674-46b5-b093-a544a0cabef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Slight variability in nuclear size shape and outline consistent with mild pleomorphism <end> There is a mild degree of crowding <end> Polarity is completely lost <end> Mitosis is infrequent <end> Nucleoli are prominent and irregular <end> High grade', 'Moderate pleomorphism and cytologic atypia is present <end> Mild nuclear crowding is seen <end> Polarity is completely lost <end> Mitosis is rare throughout the tissue <end> The nucleoli are mostly inconspicuous <end> High grade']\n"
     ]
    }
   ],
   "source": [
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a64e6cd8-7dfd-4764-ad56-9ce4cab42138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50,  51,  13,  28,  52,  53,  35,  54,  56,  57,   2,   3,   1,   7,\n",
      "           4,  43,   2,  78,  11,  12,   1,  16,   4,  60,  21,   1,  22,   4,\n",
      "         108,   1,  26,   8,  64,  35, 103,   1,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [ 85,   3,  35,  44,  45,   4,   5,   1,   2,  28,  12,   4,  68,   1,\n",
      "          16,   4,  60,  21,   1,  22,   4,  23,  73,  14,  74,   1,  14,  26,\n",
      "           8,  67,  25,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "print(caption_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e74e5-c1f5-4938-973e-d1b47e75afa7",
   "metadata": {},
   "source": [
    "### Model Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d12e3cb-462b-4819-bbb4-bc996cc56da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z5313504/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/z5313504/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'gnn_param': {\n",
    "        'cell_layers': 2,\n",
    "        'tissue_layers': 2,\n",
    "        'aggregate_method': 'sum',  # or any other method you prefer\n",
    "        'hidden_size': 256,\n",
    "        'output_size': 128,\n",
    "        'cell_conv_method': \"GCN\", # GCN, GAT, GraphSage, GIN, PNA\n",
    "        'tissue_conv_method': \"GCN\",\n",
    "    },\n",
    "    'lstm_param': {\n",
    "        'size': 128,\n",
    "        'bi_direction': True,\n",
    "        'dropout': 0.2,\n",
    "        'num_layers': 2,\n",
    "    },\n",
    "    'global_class_param': {\n",
    "        'hidden_size': 512,\n",
    "        'output_size': 128,\n",
    "        'dropout_rate': 0.1,\n",
    "    },\n",
    "    'classifier_param': {\n",
    "        'hidden_size': 128,\n",
    "        'num_class': 10,  # adjust according to your task\n",
    "        'dropout_rate': 0.3,\n",
    "    },\n",
    "    'batch_size': 32,  # adjust according to your needs\n",
    "}\n",
    "vocab_size = 119\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = DEVICE\n",
    "# Create Encoder, Attention, Decoder, Global Feature Extractor, and Classifier instances using the args\n",
    "encoder = GNNEncoder(\n",
    "    args=args,\n",
    "    cg_layer=args['gnn_param']['cell_layers'],\n",
    "    tg_layer=args['gnn_param']['tissue_layers'],\n",
    "    aggregate_method=args['gnn_param']['aggregate_method'],\n",
    "    input_feat=514,\n",
    "    hidden_size=args['gnn_param']['hidden_size'],\n",
    "    output_size=args['gnn_param']['output_size'],\n",
    ").to(device)\n",
    "\n",
    "attention = EncoderLayer(\n",
    "    d_model=args['gnn_param']['output_size'],\n",
    "    nhead=4,\n",
    "    dim_feedforward=1024,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "decoder = TransformerDecoder(\n",
    "        vocab_size = vocab_size,\n",
    "        d_model = args['gnn_param']['output_size']+args[\"global_class_param\"][\"output_size\"],\n",
    "        nhead = 2, \n",
    "        num_layers = 3, \n",
    "        dim_feedforward=2048, \n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "\n",
    "global_feature_extractor = GlobalFeatureExtractor(\n",
    "    hidden_size=args[\"global_class_param\"][\"hidden_size\"],\n",
    "    output_size=args[\"global_class_param\"][\"output_size\"],\n",
    "    dropout_rate=args[\"global_class_param\"][\"dropout_rate\"]\n",
    ").to(device)\n",
    "\n",
    "classifier = Classifier(\n",
    "    graph_output_size=args['gnn_param']['output_size'],\n",
    "    global_output_size=args[\"global_class_param\"][\"output_size\"],\n",
    "    hidden_size=args[\"classifier_param\"][\"hidden_size\"],\n",
    "    num_class=args[\"classifier_param\"][\"num_class\"],\n",
    "    dropout_rate=args[\"classifier_param\"][\"dropout_rate\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026c556b-f449-4f99-a768-43143d0bac3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128])\n",
      "min index in graph enc: -0.2489909678697586, max index in tgt: 0.25518858432769775\n",
      "min index in global: 0.0, max index in tgt: 2.020029306411743\n",
      "before cat, out torch.Size([2, 128]) global torch.Size([2, 128])\n",
      "torch.Size([2, 256])\n",
      "min index in tgt: -0.2489909678697586, max index in tgt: 2.020029306411743\n",
      "out shape torch.Size([2, 256]) cap tok shape torch.Size([2, 90])\n",
      "memory shape torch.Size([2, 256]) tgt shape torch.Size([2, 90])\n",
      "torch.Size([90, 2, 119])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n(sequence_length, batch_size, d_model)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "cg = cg.to(DEVICE)\n",
    "tg = tg.to(DEVICE)\n",
    "labels = labels.to(DEVICE)\n",
    "images = images.to(DEVICE)\n",
    "attention_masks = attention_masks.to(DEVICE)\n",
    "out = encoder(cg,tg,assign_mat,images) # (batch_size, 1, embedding)\n",
    "out = F.normalize(out, p=2, dim=1)\n",
    "print(out.shape)\n",
    "print(f\"min index in graph enc: {torch.min(out)}, max index in tgt: {torch.max(out)}\")\n",
    "global_feat = global_feature_extractor(images)\n",
    "print(f\"min index in global: {torch.min(global_feat)}, max index in tgt: {torch.max(global_feat)}\")\n",
    "print(f\"before cat, out {out.shape} global {global_feat.shape}\")\n",
    "merged_feat = torch.cat((out, global_feat), dim=1)\n",
    "\n",
    "print(merged_feat.shape)\n",
    "out = merged_feat\n",
    "# Add debug print statements\n",
    "\n",
    "print(f\"min index in tgt: {torch.min(out)}, max index in tgt: {torch.max(out)}\")\n",
    "print(f\"out shape {out.shape} cap tok shape {caption_tokens.shape}\")\n",
    "# out = attention(merged_feat)\n",
    "lstm_out = decoder(out,caption_tokens)\n",
    "print(lstm_out.shape)\n",
    "'''\n",
    "(sequence_length, batch_size, d_model)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd321f03-72f4-40de-ac8f-dd57a20021f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90, 2, 119])\n",
      "torch.Size([2, 119])\n"
     ]
    }
   ],
   "source": [
    "print(lstm_out.shape)\n",
    "a= torch.argmax(lstm_out,dim=0)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43243fd1-ec48-43bd-bd16-0b9e34514d98",
   "metadata": {},
   "source": [
    "## Dealing with Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8c926-780c-480b-80d7-92b163fcff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = \"train\"\n",
    "# _,dataset = make_dataloader(\n",
    "#     batch_size = 4,\n",
    "#     split = split,\n",
    "#     base_data_path = \"../../../../../../srv/scratch/bic/peter/Report\",\n",
    "#     graph_path = \"../../../../../../srv/scratch/bic/peter/full-graph\",\n",
    "#     vocab_path = \"vocab_bladderreport.pkl\",\n",
    "#     shuffle=True,\n",
    "#     num_workers=2,\n",
    "#     load_in_ram = True\n",
    "# )\n",
    "# class_weights = []\n",
    "# sample_weights = [0]*len(dataset)\n",
    "# print(f\"length dataset is {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f28613-4a7b-49c5-85d0-2ca054e232b9",
   "metadata": {},
   "source": [
    "### WeightRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c756a94-0134-47ea-96ee-cafc42da43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = {\n",
    "    '0': 0,\n",
    "    '1': 0,\n",
    "    '2': 0,\n",
    "    }\n",
    "\n",
    "count = 0\n",
    "class_weight = []\n",
    "for idx,output in enumerate(dataset):\n",
    "    _, _, _, _, labels, _, _,_= output\n",
    "    if count % 500 == 0 :\n",
    "        print(count)\n",
    "    class_count[str(labels)] += 1\n",
    "    count += 1\n",
    "\n",
    "print(class_count)\n",
    "for key,value in class_count.items():\n",
    "    class_weight.append(1/value)\n",
    "print(class_weight)\n",
    "sample_weights = [0]*len(dataset)\n",
    "\n",
    "for idx, output in enumerate(dataset):\n",
    "    _, _, _, _, labels, _, _,_= output\n",
    "    # print(labels.shape)\n",
    "    class_count[str(labels)] += 1\n",
    "    sample_weights[idx] = class_weight[labels]\n",
    "# print(sample_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "550f07be-f51b-45a0-8aae-fb5ba01f1c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.utils import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d79ee340-35bf-4313-970c-bd2d851fb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "from dataloader import dataset_to_loader\n",
    "class_weight = [2,1,1]\n",
    "sampler = WeightedRandomSampler(weights = sample_weights,num_samples = len(dataset),replacement = True)\n",
    "# print(list(sampler))\n",
    "with io.capture_output() as captured:\n",
    "    dl = dataset_to_loader(dataset,batch_size = 8,sampler = sampler, num_workers = 0)\n",
    "# print(dl)\n",
    "dl_count = []\n",
    "\n",
    "for idx, output in enumerate(dl):\n",
    "    # print(output)\n",
    "    # break\n",
    "    # print(cap.shape)\n",
    "    # print(lab)\n",
    "    _, _, _, _, lab, cap, _,_ = output\n",
    "    print(lab)\n",
    "    print(cap[0])\n",
    "    dl_count.extend([i.item() for i in lab])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bd21b19-1617-400b-af54-81366e2ff3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 797, 2: 773, 0: 766})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "class_distribution = Counter(dl_count)\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d73035a-1f50-478a-976d-13057c4e496f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32f10077-bd5f-4337-98aa-c702c36f1fa1",
   "metadata": {},
   "source": [
    "## Vocabluary File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cc28ed8-c2ea-4271-b125-1270d807675f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from Vocabulary import Vocabulary\n",
    "\n",
    "vocab_file= \"vocab_bladderreport.pkl\"\n",
    "\n",
    "# Load vocabulary from the pickle file\n",
    "with open(vocab_file, 'rb') as file:\n",
    "    vocab = pickle.load(file)\n",
    "# print(vocab)\n",
    "# vocab.add_word(\"<start>\")\n",
    "# Print all vocabulary items\n",
    "# print(vocab.print_all_words())\n",
    "print(vocab.word2idx['<start>'])\n",
    "# with open(vocab_file, 'wb') as file:\n",
    "#     pickle.dump(vocab, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0edebcc4-2a9b-45cf-92a2-541455e1d6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "a = ['1','2']\n",
    "b = ['3']\n",
    "print(a+b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 (ipykernel)",
   "language": "python",
   "name": "python-3.10.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
